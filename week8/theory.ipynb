{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: AdaBoost and VC-dimension\n",
    "In this exericse, we will investigate the VC-dimension of voting classifiers and argue that one has to be careful with the number of iterations of boosting to guarantee good generalization performance with high probability.\n",
    "\n",
    "The data we will consider is one-dimensional, i.e. a single feature. Our weak learner is decision stumps, i.e. a base hypothesis $h$ can be specified by a splitting value $v$, and two leaf values $a, b \\in \\{-1,1\\}$. Given a one-dimensional feature vector $x = (x_1)$, the hypothesis $h$ returns $a$ when $x_1 \\leq v$ and it returns $b$ otherwise. We let $H$ denote the set of all such decision stumps, i.e. $H$ is the set of base hypotheses.\n",
    "\n",
    "### VC-dimension of base classifier\n",
    "1. Show that the VC-dimension of $H$ is $2$.\n",
    "\n",
    "### SVAR  \n",
    "Med to punkter kan man altid klassificere korrekt: Enten er v før, imellem eller efter punkterne, og i de to tilfælde det findes,\n",
    "hvor v er imellem kan a og b skifte efter behov.  \n",
    "Med tre punkter kan tilfældet 1, -1, 1 ikke klassificeres korrekt af en stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VC-dimension of voting classifiers\n",
    "Let $C$ be the class of all voting classifiers over $H$, i.e. $C$ consists of all hypotheses of the form:\n",
    "$$\n",
    "f(x) = \\textrm{sign}(\\sum_{h \\in H} \\alpha_h h(x))\n",
    "$$\n",
    "We will show that $C$ has infinite VC-dimension. To do so, we will argue that for any size $n$, there is a set of $n$ points $x_1,\\dots,x_n$ in 1d that can be shattered, i.e. for any labeling $y \\in \\{-1,1\\}^n$ of the $n$ points, there is a hypothesis $g$ in $C$ such that $g(x_i)=y_i$ for all $i$. \n",
    "\n",
    "2. First argue that for any interval $[a, b]$ and value $z \\in \\{-1,1\\}$, it is possible to find decision stumps $h_1,h_2$ such that for $x \\in (a, b]$, we have $h_1(x)+h_2(x) = 2z$ and for any $x \\notin (a, b]$, we have $h_1(x) + h_2(x) = 0$.\n",
    "\n",
    "3. Use what you showed in 2. to come up with a 1d data set of size $n$ that you can shatter, i.e. generate any possible dichotomy/labeling using a voting classifier from $C$.\n",
    "\n",
    "### SVAR\n",
    "2. If we have one labeled point in the interval $[a,b]$ and two stumps then we can let the $v_i$s be exactly the bounds a and b, \n",
    "and let the right most stump return positive to the right of a and an the left most return positive to the left of b.\n",
    "\n",
    "3. Now we know that for any number of points we can let encapsulate them in intervals and use the above approach to shatter each. (This is a naive approach and can be optimized.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growth function for few base learners\n",
    "Finally, consider if we restrict ourselves to only using voting classifiers that combine the predictions of no more than $T$ base hypotheses. To simplify the exercise, we will even assume that we use precisely the coefficients $\\alpha_h=1$ for the $T$ hypotheses. That is, we consider the set of voting classifiers $C^T$ containing all hypotheses of the form:\n",
    "$$\n",
    "f(x) = \\textrm{sign}(\\sum_{i=1}^T h_i(x))\n",
    "$$\n",
    "where $h_1,\\dots,h_T$ are hypotheses from $H$.\n",
    "\n",
    "Recall that we have generalization bounds in terms of the growth function $m_H(n)$:\n",
    "$$\n",
    "\\Pr[|E_{out}(h^*)-E_{in}(h^*)| > \\varepsilon] < 8m_H(2n)e^{-\\varepsilon^2 n/8}.\n",
    "$$\n",
    "\n",
    "4. What is the growth function $m_H(n)$ when $H$ is the set of decision stumps on 1d data?\n",
    "\n",
    "5. What is a good upper bound on the growth function $m_{C^T}(n)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVAR\n",
    "4. Hvor mange dikotomier for n forskellige punkter (growth function)? \n",
    "Det er 2n (= (n-1)*2 + 2).\n",
    "\n",
    "5. $(2n)^T$ : For hver learner vi tilføjer lægger vi én til i eksponenten. (Så jo flere learners, vi tilføjer, desto være bliver vores chancer for gode fits uden for vores træningsdata.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implementing AdaBoost\n",
    "In this exercise your task is to implement AdaBoost as described in the lecture and the Boosting note.\n",
    "We have provided starter code in adaboost.py. See the boosting note for a description of the algorithm.\n",
    "\n",
    "You must implement the methods\n",
    "- ensemble_output\n",
    "- exp_loss\n",
    "- predict\n",
    "- score\n",
    "- fit\n",
    "\n",
    "in that order\n",
    "\n",
    "To test your implementation, run adaboost.py\n",
    "\n",
    "You should get a final accuracy of around 0.886\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Gradient Boosting by Hand\n",
    "In this exercise you must complete one step of gradient boosting with exponential loss on a small 1d data set (X, y) as shown below. The exponential loss is $L(h(x),y) = e^{-yh(x)}$.\n",
    "\n",
    "$X = [1,2,5,3,4]^\\intercal$\n",
    "\n",
    "$y = [1,1,1,-1, -1]^\\intercal$\n",
    "\n",
    "Assume that we initialize our ensemble model with the constant function $h(x) = 1$\n",
    "\n",
    "\n",
    "**Your task requires the following three steps** \n",
    "1. Compute the \"gradient\" vector $\\hat{y}$ (as in the slides) that we need to fit.\n",
    "2. Construct the best Regression Stump for fitting $\\hat{y}$.\n",
    "3. Optimize the leaf values such that the newly added tree minimize the exponential loss, with the condition that the value the leaf returns is in the interval [-1, 1].\n",
    "   What happens if we do not have this constraint that the leaf must return values in [-1, 1]?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVAR\n",
    "\n",
    "1. $\\hat{y}$  \n",
    "$\\frac{\\delta L (F_{t-1}(x_{1}),y_{1})}{\\delta F_{t-1}(x_{1})} = -ye^{-yF_{t-1}(x_{1})}$  \n",
    "And since h(x) = 1:  \n",
    "$ y_{i} = -ye^{-y}$  \n",
    "Så $ \\hat{y} = [-0.3679, -0.3679, -0.3679, 2.718, 2.718]$ \n",
    "\n",
    "2. Regression Stump (brug X og $\\hat{y}$)  \n",
    "The best tree checks $x \\leq 2$:\n",
    "We know that the mean minimizes Least Squares Loss:  \n",
    "LeftMean $ = \\frac{2 * -0.3679}{2} = -0.3679$  \n",
    "RightMean $ = \\frac{-0.3679 + (2 * 2.718) }{3} = 1.451 $  \n",
    "Now we can calculate the loss in each leaf:\n",
    "LeftLoss $ = (-0.3679 + 0.3679) + (-0.3679 + 0.3679) = 0$    \n",
    "RightLoss $ = (-0.3679 - 1.451)^2 + (2.718 - 1.451)^2 + (2.718 - 1.451)^2 = 6.52$  \n",
    "Loss = LeftLoss + RightLoss = $ 6.52 $ \n",
    "\n",
    "3. Now we optimize what the leaves return but now minimizing L and using X and y.\n",
    "Find the derivative of $ L = e^{-yF(x)}$ regarding $F(x)$.  \n",
    "Remember: $F(X) = h_{1}(X) + h_{2}(X)$ by now, where $h_{1}(x_i) = 0$.  \n",
    "So we get  \n",
    "RightLeaf = $ \\frac{\\delta e^{-(1+R)} + 2 * e^{(1+R)}}{\\delta R} = 0$  \n",
    "$\\implies R =  \\text{floor}(1.35) = 1$  \n",
    "LeftLeaft =  $ \\frac{\\delta 2 * e^{-(1+R)} }{\\delta R} = 0$  \n",
    "$\\implies R =  \\text{floor}(-\\infty) = -1 $ \n",
    "\n",
    "\n",
    "\n",
    "Fremgang generelt:\n",
    "1. Beregn $\\hat{y}$ med opmitmering af din loss function.\n",
    "2. Find optimalt split point med least squares med X og $\\hat{y}$.\n",
    "3. Find optimale gæt i blade for at minimere den loss function.\n",
    "(Go to algoritme, hvis man har mindre end 1 mill. data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Implementing Gradient Boosting\n",
    "In this exercise your task is to implement the Gradient Boosting algorithm for regression using Least Squares Loss.\n",
    "We have provided starter code in **gradientboost.py**. \n",
    "\n",
    "You must implement the methods\n",
    "- predict\n",
    "- score\n",
    "- fit\n",
    "\n",
    "in that order.\n",
    "\n",
    "**Note**: To simplify the implementation, we will **not** require that the first hypothesis you train is a constant function. Instead, just train a regression tree and give it an $\\alpha$ of $1$.\n",
    "\n",
    "\n",
    "Notice that fit gets two sets of data and labels X, y and X_val, y_val.\n",
    "The latter X_val, y_val is a separate validation test set you must test your current ensemble on in each iteration so we can plot the development on data not used for training.\n",
    "\n",
    "To test your implementation, run gradientboost.py -max_depth 1\n",
    "\n",
    "You can provide different max_depth of the base learner which is a Regression Tree (1 is default).\n",
    "\n",
    "With a default base learner with max depth 1 the mean least squares error on both training and test data should be around 0.35. \n",
    "If you change random state then the results may be different.\n",
    "\n",
    "If you increase the max_depth the results will change.  Try for instance max_depth 3 and 5 as well. What do you see?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8d4b9268494a7114fa23504c12210f1654a77f90781bc4d080d84c52239a70e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
